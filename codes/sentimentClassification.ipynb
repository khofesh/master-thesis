{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "# precision/recall\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text data is already cleaned\n",
    "inputfile = './csvfiles/output_sentiment.csv'\n",
    "review = pd.read_csv(inputfile, skip_blank_lines=False)\n",
    "review = review[['text', 'ovsentiment']]\n",
    "# exclude NaN in 'text' column (count: 11248)\n",
    "review = review[~pd.isna(review['text'])]\n",
    "X = review['text'].values\n",
    "y = review['ovsentiment'].values\n",
    "\n",
    "# len(review[review['ovsentiment'] == -1])\n",
    "# total number of -1 (negative review) is 12566\n",
    "\n",
    "# len(review[review['ovsentiment'] == 0])\n",
    "# total number of 0 (neutral) is 135292\n",
    "\n",
    "# len(review[review['ovsentiment'] == 1])\n",
    "# total number of 1 (positive review) is 455689\n",
    "\n",
    "# splitting the dataset into the training set and test set\n",
    "# stratify=y --> stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "        random_state=42, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZATION\n",
    "# see psentiment.py, tokenization is done using\n",
    "# snowball englishstemmer\n",
    "# Here, we only need to split the text\n",
    "##############################################\n",
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020820250949801757"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -1\n",
    "len(review[review['ovsentiment'] == -1])/len(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22416149860739926"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0\n",
    "len(review[review['ovsentiment'] == 0])/len(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.755018250442799"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "len(review[review['ovsentiment'] == 1])/len(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "param_grid = [{'vect__ngram_range': [(1,1)],\n",
    "    'vect__stop_words': [None],\n",
    "    'vect__tokenizer': [str.split],\n",
    "    'clf__penalty': ['l2'],\n",
    "    'clf__C': [1.0, 10.0, 15.0, 20.0, 30.0, 40.0, \n",
    "        50.0, 60.0, 70.0, 80.0, 90.0, 100.0],\n",
    "    'clf__solver': ['newton-cg', 'saga', 'lbfgs']\n",
    "    }, \n",
    "    {'vect__ngram_range': [(1,1)],\n",
    "        'vect__stop_words': [None],\n",
    "        'vect__tokenizer': [str.split],\n",
    "        'vect__use_idf':[False],\n",
    "        'vect__norm': [None],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__C': [1.0, 10.0, 15.0, 20.0, 30.0, 40.0, \n",
    "            50.0, 60.0, 70.0, 80.0, 90.0, 100.0],\n",
    "        'clf__solver': ['newton-cg', 'saga', 'lbfgs']\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tfidf = Pipeline([\n",
    "    ('vect', tfidf),\n",
    "    ('clf', LogisticRegression(\n",
    "        random_state=42,\n",
    "        multi_class='multinomial',\n",
    "        class_weight={-1:3.,\n",
    "                     0:1.5,\n",
    "                     1:1.5}\n",
    "    )\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_lr_tfidf = GridSearchCV(estimator=lr_tfidf,\n",
    "        param_grid=param_grid,\n",
    "        scoring=['accuracy', 'f1_micro'],\n",
    "        cv=5,\n",
    "        verbose=2,\n",
    "        refit='f1_micro',\n",
    "        n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.3min\n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.3min\n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.7min\n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.7min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.7min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.7min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.4min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.5min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.5min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.5min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.5min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.1min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.2min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.2min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.3min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  5.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.0min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.0min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.5min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.1min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.2min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.3min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.3min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.4min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.7min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.8min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.7min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.3min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.5min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.7min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.7min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.1min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.2min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.2min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.3min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.4min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.8min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.8min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.3min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.3min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.5min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.8min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.6min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.6min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.2min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.3min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.2min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.4min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.3min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.7min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.8min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.5min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.8min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.4min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.1min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.4min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.7min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.4min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.5min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.3min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.4min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.4min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.8min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.8min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.3min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.0min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.5min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.4min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.9min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.7min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.5min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.4min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.4min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.4min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.8min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.7min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.8min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.8min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.5min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.5min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.7min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.8min\n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.5min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.4min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.4min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.4min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.8min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.8min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.8min\n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.4min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.3min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.6min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.5min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.5min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.0min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.3min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.0min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.3min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.0min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.0min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.3min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.0min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.0min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.4min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.6min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.0min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.8min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.6min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.4min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.0min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.4min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.3min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.4min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed: 38.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.8min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.8min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.8min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.5min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.5min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.5min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.2min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.6min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.1min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.8min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.0min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.3min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.3min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.0min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.0min\n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.4min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.5min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.5min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.7min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.0min\n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.1min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.1min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.7min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.4min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.0min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.4min\n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.8min\n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.5min\n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.5min\n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.6min\n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.1min\n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.4min\n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.5min\n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.6min\n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 8.2min\n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.0min\n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.0min\n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.3min\n",
      "[CV] clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.9min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.4min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.7min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.8min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.5min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.9min\n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.1min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.4min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.4min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.3min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.4min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.3min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.4min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.7min\n",
      "[CV] clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.7min\n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.8min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.8min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.9min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.4min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.5min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.7min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.8min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=10.9min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=11.5min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=11.8min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.5min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.3min\n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.5min\n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.6min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=14.0min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.6min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.3min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.6min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.6min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=15.7min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=13.5min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=14.9min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=15.0min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.5min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.6min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=16.1min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.3min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.4min\n",
      "[CV] clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.5min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=15.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=17.0min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.5min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.3min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.3min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.5min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.6min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.7min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.7min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.8min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.8min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=17.9min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=18.2min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=19.5min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=19.3min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.5min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.5min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.6min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.5min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=20.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=20.0min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.7min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.6min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=19.5min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=21.3min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.6min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.5min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=22.0min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=22.1min\n",
      "[CV] clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.6min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.6min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.6min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.6min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=30.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=26.4min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.7min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.8min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.8min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.7min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.7min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=27.0min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=26.1min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=28.6min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.6min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=27.6min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=40.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=28.2min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.6min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.7min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.8min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=23.8min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.5min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.7min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=26.5min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=26.5min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.6min\n",
      "[CV] clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.6min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.5min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=31.3min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=50.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=32.6min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.8min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.7min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.8min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.6min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=27.7min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.8min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=29.4min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=32.1min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=32.9min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=60.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=36.1min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.9min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=28.0min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.8min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.7min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=31.7min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.6min\n",
      "[CV] clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.6min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=33.6min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=35.3min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=70.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=37.3min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.9min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.7min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.7min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.7min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.7min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV] clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=33.8min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=36.8min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=39.1min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=38.9min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.9min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=80.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=43.8min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=31.2min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.8min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=33.2min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=34.4min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.5min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.6min\n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=34.9min\n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.5min\n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=saga, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.5min\n",
      "[CV]  clf__C=90.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=34.8min\n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.6min\n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.4min\n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=27.9min\n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=26.3min\n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=29.0min\n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=26.9min\n",
      "[CV]  clf__C=100.0, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=25.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 211.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...\n",
       "          random_state=42, solver='liblinear', tol=0.0001, verbose=0,\n",
       "          warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 1)], 'vect__stop_words': [None], 'vect__tokenizer': [<method 'split' of 'str' objects>], 'clf__penalty': ['l2'], 'clf__C': [1.0, 10.0, 15.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0], 'clf__solver': ['newton-cg', 'saga', 'lbfgs']}, {'vect__ngram_ra...0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0], 'clf__solver': ['newton-cg', 'saga', 'lbfgs']}],\n",
       "       pre_dispatch='2*n_jobs', refit='f1_micro',\n",
       "       return_train_score='warn', scoring=['accuracy', 'f1_micro'],\n",
       "       verbose=2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = gs_lr_tfidf.best_params_\n",
    "best_estimator = gs_lr_tfidf.best_estimator_\n",
    "result = gs_lr_tfidf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 15.0,\n",
       " 'clf__penalty': 'l2',\n",
       " 'clf__solver': 'lbfgs',\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__norm': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__tokenizer': <method 'split' of 'str' objects>,\n",
       " 'vect__use_idf': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9850573165301494\n",
      "0.9848914754369977\n",
      "0.9851089387788916\n",
      "0.9855746328935651\n",
      "0.9852743201540916\n"
     ]
    }
   ],
   "source": [
    "# PERFORMANCE MEASURE\n",
    "#####################\n",
    "# Stratified k-fold CV\n",
    "skfolds = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train):\n",
    "    clone_estimator = clone(best_estimator)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = (y_train[train_index])\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = (y_train[test_index])\n",
    "\n",
    "    clone_estimator.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_estimator.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22451537, 0.22286886, 0.22508725, 0.22405169, 0.22427952])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dumb Classifier\n",
    "from sklearn.base import BaseEstimator\n",
    "class DumbClassifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "dumb = DumbClassifier()\n",
    "cross_val_score(dumb, X_train, y_train, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:   48.2s remaining:   32.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   48.2s finished\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = cross_val_predict(best_estimator, X_train, y_train, cv=5,\n",
    "        verbose=2, n_jobs=-1)\n",
    "\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8705,   1173,    175],\n",
       "       [  1325, 104500,   2408],\n",
       "       [   298,   1627, 362626]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9854899272425269, 0.9854899272425269)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(precision_score(y_train, y_train_pred, average='micro'),\n",
    "        recall_score(y_train, y_train_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854899272425269"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train, y_train_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:   20.6s remaining:   13.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   21.0s finished\n"
     ]
    }
   ],
   "source": [
    "# TEST MODEL ON TEST DATA\n",
    "#########################\n",
    "y_test_pred = cross_val_predict(best_estimator, X_test, y_test, cv=5,\n",
    "        verbose=2, n_jobs=-1)\n",
    "\n",
    "conf_mx = confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2010,   447,    56],\n",
       "       [  373, 25922,   764],\n",
       "       [   89,   714, 90335]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9797614116477508, 0.9797614116477508)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(precision_score(y_test, y_test_pred, average='micro'),\n",
    "        recall_score(y_test, y_test_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9797614116477508"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_test_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./training/sentiment_logreg_classweight.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save fitted model to file\n",
    "joblib.dump(best_estimator, './training/sentiment_logreg_classweight.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text data is already cleaned\n",
    "inputfile = './csvfiles/output_sentiment.csv'\n",
    "review = pd.read_csv(inputfile, skip_blank_lines=False)\n",
    "review = review[['text', 'ovsentiment']]\n",
    "# exclude NaN in 'text' column (count: 11248)\n",
    "review = review[~pd.isna(review['text'])]\n",
    "X = review['text'].values\n",
    "y = review['ovsentiment'].values\n",
    "\n",
    "# len(review[review['ovsentiment'] == -1])\n",
    "# total number of -1 (negative review) is 12566\n",
    "\n",
    "# len(review[review['ovsentiment'] == 0])\n",
    "# total number of 0 (neutral) is 135292\n",
    "\n",
    "# len(review[review['ovsentiment'] == 1])\n",
    "# total number of 1 (positive review) is 455689\n",
    "\n",
    "# splitting the dataset into the training set and test set\n",
    "# stratify=y --> stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, \n",
    "        random_state=42, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(301773,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "param_grid = [{'vect__ngram_range': [(1,1)],\n",
    "    'vect__stop_words': [None],\n",
    "    'vect__tokenizer': [str.split],\n",
    "    'clf__max_depth': [5, 6, 7],\n",
    "    'clf__min_samples_leaf': [4, 5, 6],\n",
    "    'clf__min_samples_split': [5, 6, 7],\n",
    "    'clf__max_features': ['auto', 'log2', 'sqrt']\n",
    "    }, \n",
    "    {'vect__ngram_range': [(1,1)],\n",
    "        'vect__stop_words': [None],\n",
    "        'vect__tokenizer': [str.split],\n",
    "        'vect__use_idf':[False],\n",
    "        'vect__norm': [None],\n",
    "        'clf__max_depth': [5, 6, 7],\n",
    "        'clf__min_samples_leaf': [4, 5, 6],\n",
    "        'clf__min_samples_split': [5, 6, 7],\n",
    "        'clf__max_features': ['auto', 'log2','sqrt']\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tfidf = Pipeline([\n",
    "    ('vect', tfidf),\n",
    "    ('clf', DecisionTreeClassifier(\n",
    "        random_state=42,\n",
    "        class_weight={-1:5.,\n",
    "                     0:1,\n",
    "                     1:1}\n",
    "    )\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_lr_tfidf = GridSearchCV(estimator=lr_tfidf,\n",
    "        param_grid=param_grid,\n",
    "        scoring=['accuracy', 'f1_micro'],\n",
    "        cv=5,\n",
    "        verbose=2,\n",
    "        refit='f1_micro',\n",
    "        n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.0s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.0s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.0s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   15.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.6s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.1s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.6s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.6s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  1.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.6s\n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.6s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.6s\n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.1s\n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.6s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.1s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.6s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.6s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.6s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.6s\n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.6s\n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:  3.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.5s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   3.4s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=5, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.5s\n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:  6.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=6, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=auto, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.6s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=log2, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.0s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=4, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.4s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=5, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV] clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=5, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.3s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.1s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=6, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   3.2s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   2.8s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   2.6s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   2.9s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   2.1s\n",
      "[CV]  clf__max_depth=7, clf__max_features=sqrt, clf__min_samples_leaf=6, clf__min_samples_split=7, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 810 out of 810 | elapsed:  8.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...        min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best'))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 1)], 'vect__stop_words': [None], 'vect__tokenizer': [<method 'split' of 'str' objects>], 'clf__max_depth': [5, 6, 7], 'clf__min_samples_leaf': [4, 5, 6], 'clf__min_samples_split': [5, 6, 7], 'clf__max_features': ['auto', 'log2', 'sqrt']}, {'vect__ngram_range': ...af': [4, 5, 6], 'clf__min_samples_split': [5, 6, 7], 'clf__max_features': ['auto', 'log2', 'sqrt']}],\n",
       "       pre_dispatch='2*n_jobs', refit='f1_micro',\n",
       "       return_train_score='warn', scoring=['accuracy', 'f1_micro'],\n",
       "       verbose=2)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = gs_lr_tfidf.best_params_\n",
    "best_estimator = gs_lr_tfidf.best_estimator_\n",
    "result = gs_lr_tfidf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__max_depth': 7,\n",
       " 'clf__max_features': 'auto',\n",
       " 'clf__min_samples_leaf': 4,\n",
       " 'clf__min_samples_split': 5,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__stop_words': None,\n",
       " 'vect__tokenizer': <method 'split' of 'str' objects>}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7551858970110676\n",
      "0.7549995857841106\n",
      "0.7546350758015078\n",
      "0.7588560824468966\n",
      "0.7562010173479363\n"
     ]
    }
   ],
   "source": [
    "# PERFORMANCE MEASURE\n",
    "#####################\n",
    "# Stratified k-fold CV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train):\n",
    "    clone_estimator = clone(best_estimator)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = (y_train[train_index])\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = (y_train[test_index])\n",
    "\n",
    "    clone_estimator.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_estimator.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:    2.2s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = cross_val_predict(best_estimator, X_train, y_train, cv=5,\n",
    "        verbose=2, n_jobs=-1)\n",
    "\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    98,      7,   6178],\n",
       "       [   108,   1112,  66426],\n",
       "       [   193,    277, 227374]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7574700188552322, 0.7574700188552322)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(precision_score(y_train, y_train_pred, average='micro'),\n",
    "        recall_score(y_train, y_train_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7574700188552322"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train, y_train_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:    2.2s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished\n"
     ]
    }
   ],
   "source": [
    "# TEST MODEL ON TEST DATA\n",
    "#########################\n",
    "y_test_pred = cross_val_predict(best_estimator, X_test, y_test, cv=5,\n",
    "        verbose=2, n_jobs=-1)\n",
    "\n",
    "conf_mx = confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   186,      0,   6097],\n",
       "       [   252,    155,  67239],\n",
       "       [   301,     28, 227516]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7550584212026218, 0.7550584212026218)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(precision_score(y_test, y_test_pred, average='micro'),\n",
    "        recall_score(y_test, y_test_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7550584212026218"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_test_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./training/sentiment_destree_classweight.pkl']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(best_estimator, './training/sentiment_destree_classweight.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - SGDClassifier behaves like Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hence loss='hinge' and penalty='l2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text data is already cleaned\n",
    "inputfile = './csvfiles/output_sentiment.csv'\n",
    "review = pd.read_csv(inputfile, skip_blank_lines=False)\n",
    "review = review[['text', 'ovsentiment']]\n",
    "# exclude NaN in 'text' column (count: 11248)\n",
    "review = review[~pd.isna(review['text'])]\n",
    "X = review['text'].values\n",
    "y = review['ovsentiment'].values\n",
    "\n",
    "# len(review[review['ovsentiment'] == -1])\n",
    "# total number of -1 (negative review) is 12566\n",
    "\n",
    "# len(review[review['ovsentiment'] == 0])\n",
    "# total number of 0 (neutral) is 135292\n",
    "\n",
    "# len(review[review['ovsentiment'] == 1])\n",
    "# total number of 1 (positive review) is 455689\n",
    "\n",
    "# splitting the dataset into the training set and test set\n",
    "# stratify=y --> stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "        random_state=42, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(482837,)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "param_grid = [\n",
    "        {'vect__ngram_range': [(1,1)],\n",
    "            'vect__stop_words': [None],\n",
    "            'vect__tokenizer': [str.split],\n",
    "            'clf__loss': ['hinge'],\n",
    "            'clf__penalty': ['l2'],\n",
    "            'clf__max_iter': [200, 300, 400, 500, 600]\n",
    "            }, \n",
    "        {'vect__ngram_range': [(1,1)],\n",
    "            'vect__stop_words': [None],\n",
    "            'vect__tokenizer': [str.split],\n",
    "            'vect__use_idf':[False],\n",
    "            'vect__norm': [None],\n",
    "            'clf__loss': ['hinge'],\n",
    "            'clf__penalty': ['l2'],\n",
    "            'clf__max_iter': [200, 300, 400, 500, 600]\n",
    "            }\n",
    "    ]\n",
    "\n",
    "lr_tfidf = Pipeline(\n",
    "        [   ('vect', tfidf),\n",
    "            ('clf', SGDClassifier(random_state=42, \n",
    "                                  class_weight={-1:3,\n",
    "                                               0:1.5,\n",
    "                                               1:1.5}))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(estimator=lr_tfidf,\n",
    "        param_grid=param_grid,\n",
    "        scoring=['accuracy', 'f1_macro', 'f1_micro'],\n",
    "        cv=5,\n",
    "        verbose=2,\n",
    "        refit='f1_micro',\n",
    "        n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.7min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.7min\n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.7min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.5min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.5min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.5min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.5min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.5min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.3min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  6.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.3min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.3min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.3min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.5min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.6min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.5min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.5min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.6min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.3min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.3min\n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.2min\n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.9min\n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.9min\n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.0min\n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.0min\n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.3min\n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.4min\n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.8min\n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.6min\n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 16.2min finished\n"
     ]
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "best_parameters = gs_lr_tfidf.best_params_\n",
    "best_estimator = gs_lr_tfidf.best_estimator_\n",
    "result = gs_lr_tfidf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__loss': 'hinge',\n",
       " 'clf__max_iter': 400,\n",
       " 'clf__penalty': 'l2',\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__norm': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__tokenizer': <method 'split' of 'str' objects>,\n",
       " 'vect__use_idf': False}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9753958309602461\n",
      "0.9763482727197416\n",
      "0.9759858338165852\n",
      "0.9759024915601765\n",
      "0.9760164032889422\n"
     ]
    }
   ],
   "source": [
    "# PERFORMANCE MEASURE\n",
    "#####################\n",
    "# Stratified k-fold CV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train):\n",
    "    clone_estimator = clone(best_estimator)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = (y_train[train_index])\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = (y_train[test_index])\n",
    "\n",
    "    clone_estimator.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_estimator.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:  2.2min remaining:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.2min finished\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(best_estimator, X_train, y_train, cv=5,\n",
    "        verbose=2, n_jobs=-1)\n",
    "\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5238,   4105,    710],\n",
       "       [   644, 102823,   4766],\n",
       "       [   203,   1201, 363147]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.975915267471217, 0.975915267471217)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(precision_score(y_train, y_train_pred, average='micro'),\n",
    "        recall_score(y_train, y_train_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975915267471217"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train, y_train_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:   23.4s remaining:   15.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   23.8s finished\n"
     ]
    }
   ],
   "source": [
    "# TEST MODEL ON TEST DATA\n",
    "#########################\n",
    "y_test_pred = cross_val_predict(best_estimator, X_test, y_test, cv=5,\n",
    "        verbose=2, n_jobs=-1)\n",
    "\n",
    "conf_mx = confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1305,  1039,   169],\n",
       "       [  159, 25729,  1171],\n",
       "       [   55,   302, 90781]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9760169000082843, 0.9760169000082843)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(precision_score(y_test, y_test_pred, average='micro'),\n",
    "        recall_score(y_test, y_test_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9760169000082843"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_test_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./training/sentiment_svm_classweight.pkl']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(best_estimator, './training/sentiment_svm_classweight.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "param_grid = [\n",
    "        {'vect__ngram_range': [(1,1)],\n",
    "            'vect__stop_words': [None],\n",
    "            'vect__tokenizer': [str.split],\n",
    "            'clf__loss': ['hinge', 'modified_huber', 'squared_hinge', 'perceptron', 'log'],\n",
    "            'clf__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "            'clf__max_iter': [200, 300, 400, 500, 600]\n",
    "            }, \n",
    "        {'vect__ngram_range': [(1,1)],\n",
    "            'vect__stop_words': [None],\n",
    "            'vect__tokenizer': [str.split],\n",
    "            'vect__use_idf':[False],\n",
    "            'vect__norm': [None],\n",
    "            'clf__loss': ['hinge', 'modified_huber', 'squared_hinge', 'perceptron', 'log'],\n",
    "            'clf__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "            'clf__max_iter': [200, 300, 400, 500, 600]\n",
    "            }\n",
    "    ]\n",
    "\n",
    "lr_tfidf = Pipeline(\n",
    "        [   ('vect', tfidf),\n",
    "            ('clf', SGDClassifier(random_state=42, \n",
    "                                  class_weight={-1:3,\n",
    "                                               0:1.5,\n",
    "                                               1:1.5}))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(estimator=lr_tfidf,\n",
    "        param_grid=param_grid,\n",
    "        scoring=['accuracy', 'f1_macro', 'f1_micro'],\n",
    "        cv=5,\n",
    "        verbose=2,\n",
    "        refit='f1_micro',\n",
    "        n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 150 candidates, totalling 750 fits\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.9min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  4.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.7min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.4min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.4min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.4min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.5min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.5min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.2min\n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.3min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.3min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.3min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.3min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.0min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.0min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.5min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.5min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.5min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.5min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.5min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.4min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.4min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.4min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.4min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed: 48.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.3min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.7min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.7min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.3min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.3min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.0min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.5min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.5min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.5min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.2min\n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.0min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.4min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.4min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.4min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.4min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.1min\n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.1min\n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.0min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.3min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.0min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.0min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.0min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.0min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.2min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.2min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.7min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.2min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.8min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 1.7min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.0min\n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.5min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.5min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.5min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.5min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.5min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.9min\n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.3min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.3min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.4min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.3min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.8min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.9min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.9min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.2min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.8min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.9min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.8min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.8min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.9min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.9min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.9min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.9min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.9min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.8min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.8min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.8min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.4min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.4min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.8min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.8min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.8min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.3min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.8min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.8min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.8min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.3min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.9min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.5min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.7min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.5min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 2.6min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.4min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.4min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.3min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.3min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.3min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.8min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.8min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.8min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.7min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.7min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.8min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.8min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.9min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 3.8min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.4min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.5min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.5min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.5min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.4min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 4.9min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.0min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.0min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.0min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.0min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed: 118.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.1min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.2min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.5min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.5min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.6min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.6min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 5.6min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.2min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.1min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.2min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.2min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.2min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.5min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.5min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.4min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.4min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.3min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.6min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.7min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.7min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.6min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 6.7min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.4min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.5min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.6min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.5min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.6min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.4min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.5min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.8min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.8min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.9min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.7min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.8min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.9min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.5min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.6min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.5min\n",
      "[CV] clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.6min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.7min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.1min\n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.2min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.1min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.0min\n",
      "[CV] clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.0min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.0min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.0min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.9min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.9min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.0min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.0min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.2min\n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.6min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.6min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.6min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.7min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.5min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.6min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.6min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.5min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.6min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.6min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.5min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.4min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.4min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.4min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.4min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.4min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.4min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.4min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.4min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.4min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.3min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.1min\n",
      "[CV] clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.5min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.4min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.4min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.4min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.0min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=modified_huber, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.4min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.5min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.5min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.5min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.5min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.6min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.2min\n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.3min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.1min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.3min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.3min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.3min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.4min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.3min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.4min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.4min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.4min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.4min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.5min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.5min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.5min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.4min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.3min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.5min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.3min\n",
      "[CV] clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.3min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.4min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.4min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.9min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.3min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.4min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.4min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.4min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.7min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 1.8min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.4min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.3min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=squared_hinge, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.3min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.5min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.5min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.5min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.5min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.5min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed: 219.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.3min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.3min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.3min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.3min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.2min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.2min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.2min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.0min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.1min\n",
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.1min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.0min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.1min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.0min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.0min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.4min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.4min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.9min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.0min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.1min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.4min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.0min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.0min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.0min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.4min\n",
      "[CV] clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.4min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=perceptron, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.0min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.7min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.7min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.6min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.7min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.8min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.7min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.7min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.7min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=200, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 2.7min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.5min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.4min\n",
      "[CV] clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.5min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.5min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.9min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.9min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.0min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.9min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.8min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.0min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.0min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.0min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.9min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=300, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 3.9min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.5min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.5min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.4min\n",
      "[CV] clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.5min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 4.5min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.1min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.3min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.3min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=400, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.2min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.5min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.7min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.7min\n",
      "[CV] clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.7min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.7min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.4min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.5min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.3min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.4min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.5min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.6min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.6min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.7min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.5min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=500, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.5min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.7min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.7min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.6min\n",
      "[CV] clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.6min\n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.7min\n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.4min\n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.4min\n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.3min\n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.2min\n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.6min\n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.8min\n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.6min\n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.5min\n",
      "[CV]  clf__loss=log, clf__max_iter=600, clf__penalty=elasticnet, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 750 out of 750 | elapsed: 272.2min finished\n"
     ]
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "best_parameters = gs_lr_tfidf.best_params_\n",
    "best_estimator = gs_lr_tfidf.best_estimator_\n",
    "result = gs_lr_tfidf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__loss': 'modified_huber',\n",
       " 'clf__max_iter': 600,\n",
       " 'clf__penalty': 'l1',\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__norm': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__tokenizer': <method 'split' of 'str' objects>,\n",
       " 'vect__use_idf': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9841356957201587\n",
      "0.9837213155496645\n",
      "0.9839387788915582\n",
      "0.9834413768821324\n",
      "0.9846633390634385\n"
     ]
    }
   ],
   "source": [
    "skfolds = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train):\n",
    "    clone_estimator = clone(best_estimator)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = (y_train[train_index])\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = (y_train[test_index])\n",
    "\n",
    "    clone_estimator.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_estimator.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:  4.1min remaining:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  4.1min finished\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = cross_val_predict(best_estimator, X_train, y_train, cv=5,\n",
    "        verbose=2, n_jobs=-1)\n",
    "\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8736,   1094,    223],\n",
       "       [  1772, 104005,   2456],\n",
       "       [   448,   1542, 362561]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9843943194079989, 0.9843943194079989)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(precision_score(y_train, y_train_pred, average='micro'),\n",
    "        recall_score(y_train, y_train_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9843943194079989"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train, y_train_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:   44.6s remaining:   29.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   45.3s finished\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = cross_val_predict(best_estimator, X_test, y_test, cv=5,\n",
    "        verbose=2, n_jobs=-1)\n",
    "\n",
    "conf_mx = confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2104,   347,    62],\n",
       "       [  555, 25777,   727],\n",
       "       [  171,   612, 90355]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9795045977963714, 0.9795045977963714)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(precision_score(y_test, y_test_pred, average='micro'),\n",
    "        recall_score(y_test, y_test_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9795045977963714"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_test_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./training/sentiment_sgd_classweight.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(best_estimator, './training/sentiment_sgd_classweight.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "param_grid = [\n",
    "        {'vect__ngram_range': [(1,1)],\n",
    "            'vect__stop_words': [None],\n",
    "            'vect__tokenizer': [str.split],\n",
    "            'clf__loss': ['hinge', 'squared_hinge'],\n",
    "            'clf__C': [0.1, 0.5, 1.0, 1.5],\n",
    "            'clf__shuffle': [True, False]\n",
    "            }, \n",
    "        {'vect__ngram_range': [(1,1)],\n",
    "            'vect__stop_words': [None],\n",
    "            'vect__tokenizer': [str.split],\n",
    "            'vect__use_idf':[False],\n",
    "            'vect__norm': [None],\n",
    "            'clf__loss': ['hinge', 'squared_hinge'],\n",
    "            'clf__C': [0.1, 0.5, 1.0, 1.5],\n",
    "            'clf__shuffle': [True, False]\n",
    "            }\n",
    "    ]\n",
    "\n",
    "lr_tfidf = Pipeline(\n",
    "        [   ('vect', tfidf),\n",
    "            ('clf', PassiveAggressiveClassifier(random_state=42, class_weight={-1:3,\n",
    "                                                                              0:1.5,\n",
    "                                                                              1:1.5}))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(estimator=lr_tfidf,\n",
    "        param_grid=param_grid,\n",
    "        scoring=['accuracy', 'f1_micro'],\n",
    "        cv=5,\n",
    "        verbose=2,\n",
    "        refit='f1_micro',\n",
    "        n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.0s\n",
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.0s\n",
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.1s\n",
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.4s\n",
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.5s\n",
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.0s\n",
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.8s\n",
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.5s\n",
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.4s\n",
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   9.1s\n",
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.7s\n",
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   9.0s\n",
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   6.7s\n",
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   6.8s\n",
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.8s\n",
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.4s\n",
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.9s\n",
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.0s\n",
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   31.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.6s\n",
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.5s\n",
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.9s\n",
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   9.2s\n",
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   9.2s\n",
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.2s\n",
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.4s\n",
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.6s\n",
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.2s\n",
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   6.9s\n",
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   6.7s\n",
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.8s\n",
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   9.0s\n",
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.5s\n",
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.4s\n",
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   6.6s\n",
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   6.9s\n",
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.0s\n",
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   6.8s\n",
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.2s\n",
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.9s\n",
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.6s\n",
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.8s\n",
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.8s\n",
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.1s\n",
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   6.4s\n",
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   6.5s\n",
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.3s\n",
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.4s\n",
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   9.3s\n",
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.1s\n",
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.6s\n",
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   9.3s\n",
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.3s\n",
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   6.9s\n",
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   6.9s\n",
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   9.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.3s\n",
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.2s\n",
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.3s\n",
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.5s\n",
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.0s\n",
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.0s\n",
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.3s\n",
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.9s\n",
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.1s\n",
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.7s\n",
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   6.7s\n",
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.3s\n",
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   8.2s\n",
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.3s\n",
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.0s\n",
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.2s\n",
      "[CV] clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=   7.7s\n",
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.2s\n",
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.6s\n",
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   7.0s\n",
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.6s\n",
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.2s\n",
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.6s\n",
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   7.1s\n",
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.1, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.3s\n",
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.2s\n",
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.5s\n",
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.5s\n",
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.2s\n",
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.4s\n",
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.6s\n",
      "[CV]  clf__C=0.1, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.8s\n",
      "[CV] clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.5s\n",
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.7s\n",
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   9.5s\n",
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.9s\n",
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.7s\n",
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.8s\n",
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.4s\n",
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.1s\n",
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.2s\n",
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.0s\n",
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.2s\n",
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.5s\n",
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.9s\n",
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.6s\n",
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   9.4s\n",
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   7.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.7s\n",
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.8s\n",
      "[CV] clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=0.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.5s\n",
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   7.6s\n",
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   7.9s\n",
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.1s\n",
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.2s\n",
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.4s\n",
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.7s\n",
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   7.0s\n",
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.9s\n",
      "[CV] clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.7s\n",
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.4s\n",
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   7.6s\n",
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.0s\n",
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   7.1s\n",
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.5s\n",
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   7.2s\n",
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.9s\n",
      "[CV] clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.7s\n",
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.0, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.8s\n",
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   9.1s\n",
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.8s\n",
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.1s\n",
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   7.9s\n",
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.3s\n",
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   7.0s\n",
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.4s\n",
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.6s\n",
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV] clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.9s\n",
      "[CV]  clf__C=1.5, clf__loss=hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   7.0s\n",
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.5s\n",
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   6.7s\n",
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.2s\n",
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=True, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   8.1s\n",
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   5.7s\n",
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   5.3s\n",
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   5.0s\n",
      "[CV]  clf__C=1.5, clf__loss=squared_hinge, clf__shuffle=False, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed:  3.4min finished\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "best_parameters = gs_lr_tfidf.best_params_\n",
    "best_estimator = gs_lr_tfidf.best_estimator_\n",
    "result = gs_lr_tfidf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9816400708301836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9821369397730096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9817434346781543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9814738106579955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9811527866951101\n"
     ]
    }
   ],
   "source": [
    "# PERFORMANCE MEASURE\n",
    "#####################\n",
    "# Stratified k-fold CV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train):\n",
    "    clone_estimator = clone(best_estimator)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = (y_train[train_index])\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = (y_train[test_index])\n",
    "\n",
    "    clone_estimator.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_estimator.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:    5.5s remaining:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    5.8s finished\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = cross_val_predict(best_estimator, X_train, y_train, cv=5,\n",
    "        verbose=2, n_jobs=-1)\n",
    "\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7513,   2297,    243],\n",
       "       [  1374, 103494,   3365],\n",
       "       [   273,   1111, 363167]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9820581272769071, 0.9820581272769071)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(precision_score(y_train, y_train_pred, average='micro'),\n",
    "        recall_score(y_train, y_train_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9820581272769071"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train, y_train_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:    1.6s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.6s finished\n"
     ]
    }
   ],
   "source": [
    "# TEST MODEL ON TEST DATA\n",
    "#########################\n",
    "y_test_pred = cross_val_predict(best_estimator, X_test, y_test, cv=5,\n",
    "        verbose=2, n_jobs=-1)\n",
    "\n",
    "conf_mx = confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1590,   823,   100],\n",
       "       [  313, 25773,   973],\n",
       "       [   89,   399, 90650]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9776571949299975, 0.9776571949299975)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(precision_score(y_test, y_test_pred, average='micro'),\n",
    "        recall_score(y_test, y_test_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9776571949299975"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_test_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./training/sentiment_passiveagressive_classweight.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(best_estimator, './training/sentiment_passiveagressive_classweight.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "param_grid = [\n",
    "        {'vect__ngram_range': [(1,1)],\n",
    "            'vect__stop_words': [None],\n",
    "            'vect__tokenizer': [str.split],\n",
    "            'clf__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "            'clf__alpha': [0.0001, 0.001, 0.01, 1],\n",
    "            'clf__max_iter': [1000],\n",
    "            'clf__warm_start': [0.0001]\n",
    "            }, \n",
    "        {'vect__ngram_range': [(1,1)],\n",
    "            'vect__stop_words': [None],\n",
    "            'vect__tokenizer': [str.split],\n",
    "            'vect__use_idf':[False],\n",
    "            'vect__norm': [None],\n",
    "            'clf__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "            'clf__alpha': [0.0001, 0.001, 0.01, 1],\n",
    "            'clf__max_iter': [1000],\n",
    "            'clf__warm_start': [0.0001]\n",
    "            }\n",
    "    ]\n",
    "\n",
    "lr_tfidf = Pipeline(\n",
    "        [   ('vect', tfidf),\n",
    "            ('clf', Perceptron(random_state=42, class_weight={-1:3,\n",
    "                                                             0:1.5,\n",
    "                                                             1:1.5}))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(estimator=lr_tfidf,\n",
    "        param_grid=param_grid,\n",
    "        scoring=['accuracy', 'f1_micro'],\n",
    "        cv=5,\n",
    "        verbose=2,\n",
    "        refit='f1_micro',\n",
    "        n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 8.4min\n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 8.4min\n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 8.4min\n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 8.5min\n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 8.5min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 9.9min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 9.9min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 9.9min\n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 9.9min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 9.9min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=10.4min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=10.5min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 8.6min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 8.6min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 8.6min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 8.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 8.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 18.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=10.6min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=10.6min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=10.6min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=10.1min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=10.1min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=10.0min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=10.0min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=10.0min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=10.7min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 8.8min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 8.8min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 8.7min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 8.8min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=10.5min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 8.7min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=10.7min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=10.7min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=10.7min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=10.0min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 9.2min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 9.0min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 9.0min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 8.9min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 9.2min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 9.4min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 9.4min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 9.3min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 9.4min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.6min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.7min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.7min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects> \n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.6min\n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total= 7.6min\n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.9min\n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.9min\n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.6min\n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.7min\n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.7min\n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.9min\n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.1min\n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.9min\n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.2min\n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.2min\n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.7min\n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.7min\n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.6min\n",
      "[CV] clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.7min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.9min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.0001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.5min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.5min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.6min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.0min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.1min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.2min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.3min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.8min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.8min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.5min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.0min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.1min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.1min\n",
      "[CV] clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.9min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.001, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.9min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.7min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.8min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.7min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.8min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 5.7min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.5min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.5min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.5min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.7min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.7min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.0min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.1min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.0min\n",
      "[CV] clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.9min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__alpha=0.01, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.8min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=237.8min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=239.1min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=240.8min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=240.8min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.3min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=242.0min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=242.6min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.5min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.0min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 7.1min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l1, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total= 6.5min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=309.7min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=314.5min\n",
      "[CV] clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False \n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=316.9min\n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, total=318.9min\n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=235.1min\n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=191.7min\n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=280.8min\n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=224.4min\n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=203.8min\n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=205.3min\n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=178.4min\n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=l2, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=246.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=233.4min\n",
      "[CV]  clf__alpha=1, clf__max_iter=1000, clf__penalty=elasticnet, clf__warm_start=0.0001, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<method 'split' of 'str' objects>, vect__use_idf=False, total=207.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed: 559.1min finished\n"
     ]
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "best_parameters = gs_lr_tfidf.best_params_\n",
    "best_estimator = gs_lr_tfidf.best_estimator_\n",
    "result = gs_lr_tfidf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.0001,\n",
       " 'clf__max_iter': 1000,\n",
       " 'clf__penalty': 'l1',\n",
       " 'clf__warm_start': 0.0001,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__norm': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__tokenizer': <method 'split' of 'str' objects>,\n",
       " 'vect__use_idf': False}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9729726931002702\n",
      "0.9754680639549334\n",
      "0.9202945074973076\n",
      "0.9711078433403061\n",
      "0.9666342190833213\n"
     ]
    }
   ],
   "source": [
    "# PERFORMANCE MEASURE\n",
    "#####################\n",
    "# Stratified k-fold CV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train):\n",
    "    clone_estimator = clone(best_estimator)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = (y_train[train_index])\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = (y_train[test_index])\n",
    "\n",
    "    clone_estimator.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_estimator.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:  6.3min remaining:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  6.3min finished\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = cross_val_predict(best_estimator, X_train, y_train, cv=5,\n",
    "        verbose=2, n_jobs=-1)\n",
    "\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6817,   2675,    561],\n",
       "       [  2414, 101758,   4061],\n",
       "       [  2273,   2174, 360104]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9706774750070934, 0.9706774750070934)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(precision_score(y_train, y_train_pred, average='micro'),\n",
    "        recall_score(y_train, y_train_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9706774750070934"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train, y_train_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:  1.1min remaining:   44.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = cross_val_predict(best_estimator, X_test, y_test, cv=5,\n",
    "        verbose=2, n_jobs=-1)\n",
    "\n",
    "conf_mx = confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1752,   555,   206],\n",
       "       [  573, 24891,  1595],\n",
       "       [  173,  2798, 88167]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9511225250600613, 0.9511225250600613)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(precision_score(y_test, y_test_pred, average='micro'),\n",
    "        recall_score(y_test, y_test_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9511225250600613"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_test_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./training/sentiment_perceptron_classweight.pkl']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(best_estimator, './training/sentiment_perceptron_classweight.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
